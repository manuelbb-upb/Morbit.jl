<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>TaylorModels · Morbit.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://manuelbb-upb.github.io/Morbit.jl/TaylorModel/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Morbit.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../example_two_parabolas/">Two Parabolas</a></li><li><a class="tocitem" href="../constraints/">Constraints</a></li><li><a class="tocitem" href="../composites/">Composite Functions</a></li></ul></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../ExactModel/">ExactModels</a></li><li><a class="tocitem" href="../RbfModel/">RbfModels</a></li><li class="is-active"><a class="tocitem" href>TaylorModels</a><ul class="internal"><li><a class="tocitem" href="#Model-Construction"><span>Model Construction</span></a></li><li><a class="tocitem" href="#Model-Evaluation"><span>Model Evaluation</span></a></li><li><a class="tocitem" href="#taylor_summary"><span>Summary &amp; Quick Examples</span></a></li></ul></li><li><a class="tocitem" href="../LagrangeModel/">LagrangeModels</a></li></ul></li><li><span class="tocitem">Random Notebooks</span><ul><li><a class="tocitem" href="../notebooks/notebook_finite_differences/">Finite Differences</a></li><li><a class="tocitem" href="../notebooks/notebook_polynomial_interpolation/">Lagrange Interpolation</a></li></ul></li><li><a class="tocitem" href="../custom_logging/">Pretty Printing</a></li><li><span class="tocitem">Developer</span><ul><li><a class="tocitem" href="../dev_man/">DocStrings</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Models</a></li><li class="is-active"><a href>TaylorModels</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>TaylorModels</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/manuelbb-upb/Morbit.jl/blob/main/src/models/TaylorModel.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Taylor-Polynomial-Models"><a class="docs-heading-anchor" href="#Taylor-Polynomial-Models">Taylor Polynomial Models</a><a id="Taylor-Polynomial-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Taylor-Polynomial-Models" title="Permalink"></a></h1><p>This file is automatically generated from source code. For usage examples refer to <a href="#taylor_summary">Summary &amp; Quick Examples</a>.</p><p>We provide vector valued polynomial Taylor models of degree 1 or 2. They implement the <code>AbstractSurrogate</code> interface.</p><p>We allow the user to either provide gradient and hessian callback handles or to request finite difference approximations. For using callbacks, we have <code>TaylorConfigCallbacks</code>. <br/>There are two ways to use finite differences. The old (not recommended way) is to use <code>TaylorConfigFiniteDiff</code>. This uses <code>FiniteDiff.jl</code> and could potentially require more evaluations. <br/>To make use of the new 2-phase construction procedure, use <code>TaylorConfig</code> and set the fields <code>gradients</code> and <code>hessians</code> to an <code>RFD.FiniteDiffStamp</code>. If they use the same stamp (default: <code>RFD.CFDStamp(1,3) :: CFDStamp{3,Float64}</code>), it should be the most efficient, because we get the gradients for free from computing the hessians.</p><pre><code class="language-julia hljs">include(joinpath(@__DIR__,&quot;RecursiveFiniteDifferences.jl&quot;))

using .RecursiveFiniteDifferences
const RFD = RecursiveFiniteDifferences</code></pre><p>The actual model is defined only by the gradient vectors at <code>x₀</code> and the Hessians (if applicable).</p><pre><code class="language-julia hljs">@with_kw struct TaylorModel{
    XT &lt;: AbstractVector{&lt;:Real}, FXT &lt;: AbstractVector{&lt;:Real},
    G &lt;: AbstractVector{&lt;:AbstractVector{&lt;:Real}},
    HT &lt;: Union{Nothing,AbstractVector{&lt;:AbstractMatrix{&lt;:Real}}},
    } &lt;: AbstractSurrogate

    # expansion point and value
    x0 :: XT
    fx0 :: FXT

    # gradient(s) at x0
    g :: G
    H :: HT = nothing

    num_outputs = length(fx0)
end

fully_linear( :: TaylorModel ) = true
num_outputs( m :: TaylorModel ) = m.num_outputs</code></pre><p>Note, that the derivative approximations are actually constructed for the function(s)</p><p class="math-container">\[    f_ℓ ∘ s^{-1}\]</p><p>if some internal transformation <span>$s$</span> has happened before. If the problem is unbounded then <span>$s = \operatorname{id} = s^{-1}$</span>.</p><h2 id="Model-Construction"><a class="docs-heading-anchor" href="#Model-Construction">Model Construction</a><a id="Model-Construction-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Construction" title="Permalink"></a></h2><p>Because of all the possibilities offered to the user, we actually have several (sub-)implementiations of <code>AbstractSurrogateConfig</code> for Taylor Models.</p><pre><code class="language-julia hljs">abstract type TaylorCFG &lt;: AbstractSurrogateConfig end</code></pre><p>We make sure, that all subtypes have a field <code>max_evals</code>:</p><pre><code class="language-julia hljs">max_evals( cfg :: TaylorCFG ) = cfg.max_evals</code></pre><h3 id="Recursive-Finite-Difference-Models"><a class="docs-heading-anchor" href="#Recursive-Finite-Difference-Models">Recursive Finite Difference Models</a><a id="Recursive-Finite-Difference-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Recursive-Finite-Difference-Models" title="Permalink"></a></h3><p>Let&#39;s start by defining the recommended way of using Taylor approximations. The derivative information is approximated using a dynamic programming approach and we take care to avoid unnecessary objective evaluations.</p><pre><code class="language-julia hljs">@doc &quot;&quot;&quot;
    TaylorConfig(; degree, gradients :: RFD.CFDStamp, hessians :: RFD.CFDStamp, max_evals)

Configuration for a polynomial Taylor model using finite difference approximations of the derivatives.
By default we have `degree = 2` and `gradients == hessians == RFD.CFDStamp(1,2)`, that is,
a first order central difference scheme of accuracy order 3 is recursed to compute the Hessians
and the gradients.
In this case, the finite difference scheme is the same for both Hessians and gradients and we profit
from caching intermediate results.
&quot;&quot;&quot;
@with_kw struct TaylorConfig{
        S1 &lt;: RFD.FiniteDiffStamp,
        S2 &lt;: Union{Nothing,RFD.FiniteDiffStamp}
    } &lt;: TaylorCFG

    degree :: Int64 = 2

    gradients :: S1 = RFD.CFDStamp(1,2)
    hessians :: S2 = gradients

    max_evals :: Int64 = typemax(Int64)

    @assert 1 &lt;= degree &lt;= 2 &quot;Can only construct linear and quadratic polynomial Taylor models.&quot;
end

combinable( :: TaylorConfig ) = true</code></pre><p>The new meta type only stores database indices of sites used for a finite diff approximation in the actual construction call and is filled in the <code>prepare_XXX</code> methods:</p><pre><code class="language-julia hljs">@with_kw struct TaylorIndexMeta{W1, W2} &lt;: AbstractSurrogateMeta
    database_indices :: Vector{Int} = Int[]
    grad_setter_indices :: Vector{Int} = Int[]
    hess_setter_indices :: Vector{Int} = Int[]
    hess_wrapper :: W1 = nothing
    grad_wrapper :: W2 = nothing
end</code></pre><p>The end user won&#39;t be interested in the wrappers, so we put <code>nothing</code> in there:</p><pre><code class="language-julia hljs">function get_saveable_type( :: TaylorConfig )
    return TaylorIndexMeta{Nothing, Nothing}
end
get_saveable( meta :: TaylorIndexMeta ) = TaylorIndexMeta(;
    grad_setter_indices = meta.grad_setter_indices,
    hess_setter_indices = meta.hess_setter_indices
)</code></pre><p>And the configs should easily be available:</p><pre><code class="language-julia hljs">export TaylorConfig, TaylorCallbackConfig</code></pre><p>The new construction process it is a bit complicated. We set up a recursive finite diff tree and need this little helper:</p><pre><code class="language-julia hljs">&quot;Return `unique_elems, indices = unique_with_indices(arr)` such that
`unique_elems[indices] == arr` (and `unique_elems == unique(arr)`).&quot;
function unique_with_indices( x :: AbstractVector{T} ) where T
	unique_elems = T[]
	indices = Int[]
	for elem in x
		i = findfirst( e -&gt; all( isequal.(e,elem) ), unique_elems )
		if isnothing(i)
			push!(unique_elems, elem)
			push!(indices, length(unique_elems) )
		else
			push!(indices, i)
		end
	end
	return unique_elems, indices
end</code></pre><p>Now, if the polynomial degree equals 2 we construct a tree for the Hessian calculation. In any case, we need a tree for the gradients/jacobian. If the <code>RFD.FiniteDiffStamp</code> for the gradients is the same as for the Hessians, we can re-use the Hessian tree for this purpose. Else, we need to construct a new one.</p><pre><code class="language-julia hljs">function _get_RFD_trees( x, fx, grad_stamp, hess_stamp = nothing, deg = 2)
    if deg &gt;= 2
        @assert !isnothing(hess_stamp)
        # construct tree for hessian first
        hess_wrapper = RFD.DiffWrapper(; x0 = x, fx0 = fx, stamp = hess_stamp, order = 2 )
    else
        hess_wrapper = nothing
    end

    if !isnothing(hess_wrapper) &amp;&amp; grad_stamp == hess_stamp
        grad_wrapper = hess_wrapper
    else
        grad_wrapper = RFD.DiffWrapper(; x0 = x, fx0 = fx, stamp = grad_stamp, order = 1 )
    end

    return grad_wrapper, hess_wrapper
end


function prepare_init_model(cfg :: TaylorConfig, func_indices,
    mop, scal, id, sdb, ac; kwargs...)
    return prepare_update_model(nothing, TaylorIndexMeta(), cfg, func_indices, mop, scal, id, sdb, ac ; kwargs... )
end</code></pre><p>The actual database preparations are delegated to the <code>prepare_update_model</code> function.</p><pre><code class="language-julia hljs">function prepare_update_model(
    mod :: Union{Nothing, TaylorModel}, meta :: TaylorIndexMeta,
    cfg :: TaylorConfig, func_indices, mop, scal, iter_data, sdb, algo_config;
    kwargs...
)

    @logmsg loglevel2 &quot;Building Taylor model for $(func_indices)&quot;
    db = get_sub_db( sdb, func_indices )
    x = get_x_scaled( iter_data )
    x_index = get_x_index( iter_data, func_indices )
    fx = get_value( db, x_index )

    grad_wrapper, hess_wrapper = _get_RFD_trees( x, fx, cfg.gradients, cfg.hessians, cfg.degree )

    XT = typeof(x)

    lb, ub = full_bounds_internal( scal )

    if cfg.degree &gt;= 2
        RFD.substitute_leaves!(hess_wrapper)
        # We project into the scaled variable boundaries to avoid violations:
        hess_sites = [ _project_into_box(s,lb,ub) for s in RFD.collect_leave_sites( hess_wrapper ) ]
    else
        hess_sites = XT[]
    end

    # collect leave sites for gradients
    if grad_wrapper == hess_wrapper
        grad_sites = hess_sites
    else
        RFD.substitute_leaves!( grad_wrapper )
        grad_sites = [ _project_into_box(s,lb,ub) for s in RFD.collect_leave_sites( grad_wrapper ) ]
    end

    combined_sites = [ [x,]; hess_sites; grad_sites ]

    unique_new, unique_indices = unique_with_indices(combined_sites)
    # now: `combined_sites == unique_new[unique_indices]`

    num_hess_sites = length(hess_sites)
    hess_setter_indices = unique_indices[ 2 : num_hess_sites + 1]
    grad_setter_indices = unique_indices[ num_hess_sites + 2 : end ]
    # now: `hess_sites == unique_new[ hess_setter_indices ]` and
    # `grad_sites == unique_new[ grad_setter_indices ]`

    db_indices = [ [x_index,]; [ new_result!(db, ξ, []) for ξ in unique_new[ 2:end ] ] ]
    # now: `unique_new == get_site.(db, db_indices)`

    # we return a new meta object in each iteration, so that the node cache is reset in between.
    return TaylorIndexMeta(;
        database_indices = db_indices,
        grad_setter_indices,
        hess_setter_indices,
        grad_wrapper,
        hess_wrapper
    )
end</code></pre><p>If the meta data is set correctly, we only have to set the value vectors for the RFD trees and then ask for the right matrices:</p><pre><code class="language-julia hljs">function init_model(meta :: TaylorIndexMeta, cfg :: TaylorConfig, func_indices,
	mop, scal, iter_data, sdb, ac; kwargs... )

    return update_model( nothing, meta, cfg, func_indices, mop, scal, iter_data, sdb, ac; kwargs...)
end</code></pre><p>Note, that we only perform updates if the iterate has changed, <code>x != mod.x0</code>, because we don&#39;t change the differencing parameters.</p><pre><code class="language-julia hljs">function update_model( mod::Union{Nothing,TaylorModel}, meta :: TaylorIndexMeta, cfg :: TaylorConfig,
	func_indices, mop, scal, iter_data,
	sdb, ac;
	kwargs... )

    db = get_sub_db( sdb, func_indices )
    x = get_x_scaled(iter_data)
    x_index = get_x_index( iter_data, func_indices )
    fx = get_value( db, x_index )

    if isnothing(mod) || (x != mod.x0)
        all_leave_vals = get_value.( db, meta.database_indices )

        if !isnothing( meta.hess_wrapper )
            hess_leave_vals = all_leave_vals[ meta.hess_setter_indices ]
            RFD.set_leave_values!( meta.hess_wrapper, hess_leave_vals )
            H = [ RFD.hessian( meta.hess_wrapper; output_index = ℓ ) for ℓ = 1 : num_outputs(func_indices) ]
        else
            H = nothing
        end

        # calculate gradients
        if meta.hess_wrapper != meta.grad_wrapper
            grad_leave_vals = all_leave_vals[ meta.grad_setter_indices ]
            RFD.set_leave_values!( meta.grad_wrapper, grad_leave_vals )
        end

        # if hessians have been calculated before and `grad_wrapper == hess_wrapper` we profit from caching
        J = RFD.jacobian( meta.grad_wrapper )
        g = copy.( eachrow( J ) )

        return TaylorModel(;
            x0 = x,
            fx0 = fx,
            g, H
        ), meta
    else
        return mod,meta
    end
end</code></pre><h3 id="Callback-Models-with-Derivatives,-AD-or-Adaptive-Finite-Differencing"><a class="docs-heading-anchor" href="#Callback-Models-with-Derivatives,-AD-or-Adaptive-Finite-Differencing">Callback Models with Derivatives, AD or Adaptive Finite Differencing</a><a id="Callback-Models-with-Derivatives,-AD-or-Adaptive-Finite-Differencing-1"></a><a class="docs-heading-anchor-permalink" href="#Callback-Models-with-Derivatives,-AD-or-Adaptive-Finite-Differencing" title="Permalink"></a></h3><p>The old way of defining Taylor Models was to provide an objective callback function and either give callbacks for the derivatives too or ask for automatic differencing. This is very similar to the <code>ExactModel</code>s, with the notable difference that the gradient and Hessian information is only used to construct models <span>$m_ℓ = f_0 + \mathbf g^T \mathbf h + \mathbf h^T \mathbf H \mathbf h$</span> <strong>once</strong> per iteration and then use these <span>$m_ℓ$</span> for all subsequent model evaluations/differentiation.</p><pre><code class="language-julia hljs">&quot;&quot;&quot;
    TaylorCallbackConfig(;degree=1,max_evals=typemax(Int64))

Configuration for a linear or quadratic Taylor model where there are callbacks provided for the
gradients and -- if applicable -- the Hessians.
&quot;&quot;&quot;
@with_kw struct TaylorCallbackConfig &lt;: TaylorCFG

    degree :: Int64 = 1

    max_evals :: Int64 = typemax(Int64)

    @assert 1 &lt;= degree &lt;= 2 &quot;Can only construct linear and quadratic polynomial Taylor models.&quot;
end

needs_gradients( cfg :: TaylorCallbackConfig ) = true
needs_hessians( cfg :: TaylorCallbackConfig ) = cfg.degree &gt;= 2</code></pre><p>For these models, it is not advisable to combine objectives:</p><pre><code class="language-julia hljs">combinable( :: TaylorCallbackConfig ) = false</code></pre><p>The meta structs are just for show:</p><pre><code class="language-julia hljs">struct TaylorCallbackMeta &lt;: AbstractSurrogateMeta end</code></pre><p>The initialization for the legacy config types is straightforward as they don&#39;t use the new 2-phase process:</p><pre><code class="language-julia hljs">function prepare_init_model(cfg :: TaylorCallbackConfig, func_indices,
    mop, scal, id, sdb, ac; kwargs...)
    return TaylorCallbackMeta()
end</code></pre><p>The model construction happens in the <code>update_model</code> method and makes use of the <code>get_gradient</code> and <code>get_hessian</code> methods of the <code>AbstractVectorObjective</code>.</p><pre><code class="language-julia hljs">function init_model(meta :: TaylorCallbackMeta, cfg :: TaylorCallbackConfig, func_indices,
    mop, scal, id, sdb, ac; kwargs...)
    return update_model(nothing, meta, cfg, func_indices, mop, scal, id, sdb, ac; kwargs... )
end

function update_model( mod :: Union{Nothing,TaylorModel}, meta :: TaylorCallbackMeta, cfg :: TaylorCallbackConfig, func_indices,
    mop, scal, id, sdb, ac; kwargs...)

    x0 = get_x_scaled(id)
    x0_unscaled = get_x(id)</code></pre><p>if there is no model yet OR x0 has changed</p><pre><code class="language-julia hljs">    if isnothing(mod) || (x0 != mod.x0)
        fx0 = get_vals( id, sdb, func_indices )

        J = jacobian_of_unscaling(scal)
        Jᵀ = transpose(J)

        g = collect( Iterators.flatten(
            [ let func = _get(mop, ind), func_jac = _get_jacobian(func, x0_unscaled);
                [ _ensure_vec( Jᵀ * func_jac[ℓ,:] ) for ℓ = 1 : num_outputs(func) ]
            end for ind = func_indices ]
        ) )

        H = if cfg.degree &gt;= 2
            collect( Iterators.flatten(
                [ let func = _get(mop, ind);
                    [ (Jᵀ * _get_hessian(func, x0_unscaled, ℓ) * J) for ℓ = 1 : num_outputs(func) ]
                end for ind = func_indices ]
            ) )
        else
            nothing
        end

        return TaylorModel(; x0, fx0, g, H ), meta
    else
        return mod, meta
    end

end</code></pre><h2 id="Model-Evaluation"><a class="docs-heading-anchor" href="#Model-Evaluation">Model Evaluation</a><a id="Model-Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Evaluation" title="Permalink"></a></h2><p>The evaluation of a Taylor model of form</p><p class="math-container">\[    m_ℓ(\mathbf x) = f_ℓ(\mathbf x_0) +
    \mathbf g^T ( \mathbf x - \mathbf x_0 ) + ( \mathbf x - \mathbf x_0 )^T \mathbf H_ℓ ( \mathbf x - \mathbf x_0)\]</p><p>is straightforward:</p><pre><code class="language-julia hljs">&quot;Evaluate (internal) output `ℓ` of TaylorModel `tm`, provided a difference vector `h = x - x0`.&quot;
function _eval_models( tm :: TaylorModel, h :: Vec, ℓ :: Int )
    ret_val = tm.fx0[ℓ] + tm.g[ℓ]&#39;h
    if !isnothing(tm.H)
        ret_val += .5 * h&#39;tm.H[ℓ]*h
    end
    return ret_val
end

&quot;Evaluate (internal) output(s) `ℓ` of `tm` at scaled site `x̂`.&quot;
function eval_models( tm :: TaylorModel, scal :: AbstractVarScaler, x̂ :: Vec, ℓ )
    h = x̂ .- tm.x0
    return reduce( vcat, _eval_models( tm, h, l ) for l = ℓ )
 end</code></pre><p>For the vector valued model, we iterate over all (internal) outputs:</p><pre><code class="language-julia hljs">function eval_models( tm :: TaylorModel, scal :: AbstractVarScaler, x̂ :: Vec )
    h = x̂ .- tm.x0
    return eval_models(tm, scal, x̂, 1:num_outputs(tm))
end</code></pre><p>The gradient of <span>$m_ℓ$</span> can easily be determined:</p><pre><code class="language-julia hljs">function get_gradient( tm :: TaylorModel, scal :: AbstractVarScaler, x̂ :: Vec, ℓ)
    if isnothing(tm.H)
        return tm.g[ℓ]
    else
        h = x̂ .- tm.x0
        return tm.g[ℓ] .+ .5 * ( tm.H[ℓ]&#39; + tm.H[ℓ] ) * h
    end
end</code></pre><p>And for the Jacobian, we again iterate:</p><pre><code class="language-julia hljs">function get_jacobian( tm :: TaylorModel, scal :: AbstractVarScaler, x̂ :: Vec, rows = nothing )
    indices = isnothing(rows) ? eachindex(tm.g) : rows
    grad_list = [ get_gradient(tm, scal, x̂, ℓ) for ℓ = indices ]
    return transpose( hcat( grad_list... ) )
    #scr # TODO pre-allocate matrix for speed
end</code></pre><h2 id="taylor_summary"><a class="docs-heading-anchor" href="#taylor_summary">Summary &amp; Quick Examples</a><a id="taylor_summary-1"></a><a class="docs-heading-anchor-permalink" href="#taylor_summary" title="Permalink"></a></h2><ol><li>The recommended way to use Finite Difference Taylor models is to define them with TaylorConfig, i.e.,<pre><code class="language-julia hljs">add_objective!(mop, f; model_cfg = TaylorConfig(), n_out = 1)</code></pre></li><li>To use <code>FiniteDiff.jl</code> instead, do<pre><code class="language-julia hljs">add_objective!(mop, f;
   model_cfg = TaylorCallbackConfig(),
   diff_method = Morbit.FiniteDiffWrapper)</code></pre></li><li>Have callbacks for the gradients and the Hessians? Great!<pre><code class="language-julia hljs">add_objective!(mop, f;
   TaylorCallbackConfig(; degree = 1),
   gradients = [g1,g2]))</code></pre></li><li>No callbacks, but you want the correct matrices anyways? <code>ForwardDiff</code> to the rescue:<pre><code class="language-julia hljs">add_objective!(mop, f;
   model_cfg = TaylorCallbackConfig(),
   diff_method = Morbit.AutoDiffWrapper)</code></pre></li></ol><h3 id="Complete-usage-example"><a class="docs-heading-anchor" href="#Complete-usage-example">Complete usage example</a><a id="Complete-usage-example-1"></a><a class="docs-heading-anchor-permalink" href="#Complete-usage-example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using Morbit
Morbit.print_all_logs()
mop = MixedMOP(3)

add_objective!( mop, x -&gt; sum( ( x .- 1 ).^2 ); model_cfg = TaylorConfig(), n_out = 1)
add_objective!( mop, x -&gt; sum( ( x .+ 1 ).^2 ), model_cfg = TaylorConfig(), n_out = 1)

x_fin, f_fin, _ = optimize( mop, [-π, ℯ, 0])</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../RbfModel/">« RbfModels</a><a class="docs-footer-nextpage" href="../LagrangeModel/">LagrangeModels »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.17 on <span class="colophon-date" title="Friday 20 May 2022 09:21">Friday 20 May 2022</span>. Using Julia version 1.6.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
