module RBF
export RBFModel, train!, output, grad, is_valid, jac, min_num_sites

using Parameters: @with_kw, @pack!, @unpack
using LinearAlgebra: norm, Hermitian, lu, qr, cholesky, eigen, I

# If kernel `:k` is conditionally positive definite (cpd) of order `d` then
# Œ£_k c‚Çñ k(‚Äñ x - x‚Çñ‚Äñ) + p(x)
# allows for a unique interpolation if p(x) is of degree *at least* `d-1`.
cpd_order( ::Val{:exp} ) = 0 :: Int64;
cpd_order( ::Val{:multiquadric} ) = 2 :: Int64;
cpd_order( ::Val{:cubic} ) = 2 :: Int64;
cpd_order( ::Val{:thin_plate_spline} ) = typemax(Int64) :: Int64;

const RVec = Vector{R} where R<:Real;
const RVecArr = Vector{<:RVec};

@with_kw mutable struct RBFModel
    training_sites :: RVecArr = RVec[];
    training_values :: RVecArr = RVec[];
    n_in :: Int64 = length(training_sites) > 0 ? length(training_sites[1]) : -1;
    kernel :: Symbol = :multiquadric;
    shape_parameter :: Union{Real, RVec} = 1;
    fully_linear :: Bool = false;
    polynomial_degree :: Int64 = 1; # -1 means *no polynomial*, not a rational function

    rbf_coefficients :: Array{R,2} where R<:Real = Matrix{Float16}(undef,0,0);
    poly_coefficients :: Array{R,2} where R<:Real = Matrix{Float16}(undef,0,0);

    #function_handle :: Union{Nothing, Function} = nothing;      # TODO deprecate

    @assert polynomial_degree <= 1 "For now only polynomials with degree -1, 0, or 1 are allowed."
    @assert isa(shape_parameter, Real) || length(shape_parameter) == length(training_sites)
end

@doc "Return number of input variables to RBFModel `m`. If `m.n_in` is not set, infer from training sites if possible."
function n_in(m::RBFModel)
    if m.n_in < 0
        if length(m.training_sites) > 0
            m.n_in = length(m.training_sites[0])
        else
            error( "Could not determine number of input variables for RBFModel and 
            infer minimum number of interpolation sites.")
        end
    end
    return m.n_in
end

# minimal number of `training_sites` a RBFModel with polynomial tail must have
# (equals number of different sites so that Œ† has full column rank)
function min_num_sites( m :: RBFModel )
    pd = m.polynomial_degree;
    if pd == -1
        return 0
    elseif pd == 0
        return 1
    else
        return n_in(m) + 1
    end
end

num_outputs( m::RBFModel ) = length( m.training_values[end] );

@doc "Return `true` if RBFModel `m` conforms to the requirements by [WILD]."
is_valid( m :: RBFModel ) = m.polynomial_degree >= cpd_order( Val(m.kernel) ) - 1 &&
                            length(m.training_sites) >= min_num_sites(m);

Broadcast.broadcastable(m::RBFModel) = Ref(m);

function (m::RBFModel)(eval_site:: RVec)
    output(m, eval_site)  # return an array of arrays for ease of use in other methods
end

# === Evaluate RBF part of the Model ===
kernel( ::Val{:exp}, r::Real, s::Real = 1 ) = exp(-(r*s)^2);
kernel( ::Val{:multiquadric}, r::Real, s::Real = 1 ) = - sqrt( 1 + (s*r)^2);
kernel( ::Val{:cubic}, r::Real, s::Real = 1 ) = (r*s)^3;     # TODO better change this to r^s, s ‚àà (2,4)
function kernel( ::Val{:thin_plate_spline}, r::Real, s::Real = 1)
    r == 0 ? 0 : (-1)^(s+1) * r^(2*s) * log(r)
end

@doc "Return n_sites-array with difference vectors (x_1 - c_{1,m}, ‚Ä¶, x_n - c_{n,m}) of length n."
function x_minus_sites( m:: RBFModel, x :: RVec)
    [ x .- site for site ‚àà m.training_sites ]
end

@doc "Return vector of all distance values between x and each center of m"
function center_distances( m :: RBFModel, x :: RVec )
    r_vector = norm.( x_minus_sites( m, x), 2 )
end

@doc "Evaluate all ``n_c`` basis functions of m::RBFModel at second argument x
and return ``n_c``-Array"
function œÜ( m::RBFModel, x :: RVec )
    kernel.( Val(m.kernel), center_distances(m, x), m.shape_parameter )
end

@doc "Symmetric matrix of every center evaluated in all basis functions."
function get_Œ¶( m::RBFModel )
    hcat( œÜ.(m, m.training_sites)... )'
end

@doc "Return ‚Ñì-th output of the RBF Part of the model at site x."
function rbf_output( m::RBFModel, ‚Ñì :: Int64, x :: RVec)
    œÜ(m, x)'m.rbf_coefficients[:, ‚Ñì]
end

@doc "Return k-vector of *all* RBF model outputs at site x."
function rbf_output( m::RBFModel, x :: RVec )
    vec(œÜ(m, x)'m.rbf_coefficients) # wrapped in vector for addition with polynomial part
end

# d/dr kernel
‚àÇkernel( ::Val{:exp}, r::Real, s::Real = 1 ) = - 2 * r * s^2 * kernel( Val(:exp), r, s )
‚àÇkernel( ::Val{:multiquadric}, r::Real, s::Real = 1 ) = r * s^2 / kernel( Val(:multiquadric), r, s ) ;
‚àÇkernel( ::Val{:cubic}, r::Real, s::Real = 1 ) = 3 * s^3 * r^2 ;
‚àÇkernel( ::Val{:thin_plate_spline}, r::Real, s::Real = 1 ) = r == 0 ? 0 :
    (-1)^(s+1) * r^(2*s-1) * (
        1 +
        2 * s * log(r)
    );
#=
@doc "Return an n_vars x n_sites matrix where column j is an n-vector of entries 2 coeff(j,‚Ñì) *(x_1 - c_{1,j}), ‚Ä¶, 2*(x_n - c_{n,j}). "
function grad_prefix( m :: RBFModel, ‚Ñì :: Int64, x :: RVec )
    differences = x_minus_sites( m, x )
    2 .* hcat( [  m.rbf_coefficients[ i, ‚Ñì ] * differences[i] for i = eachindex(differences) ] ... )
end

@doc "Return an k-array of n_vars x n_sites matrices (1 for each RBF model output) where each column is an n-vector of entries 2 coeff(m,‚Ñì) *(x_1 - c_{1,m}), ‚Ä¶, 2*(x_n - c_{n,m}). "
function grad_prefix( m :: RBFModel, x :: RVec )
    differences = x_minus_sites( m, x )
    grad_prefix_matrices = [];
    for ‚Ñì = 1 : length(m.training_values[1])
        grad_pref_‚Ñì = 2 .* hcat( [  m.rbf_coefficients[ i, ‚Ñì ] * differences[i] for i = eachindex(differences) ] ... )
        push!(grad_prefix_matrices, grad_pref_‚Ñì)
    end
    return grad_prefix_matrices, differences
end
=#

@doc "Compute gradient term of ‚Ñì-th RBF (scalar) model output and return n-vector."
function rbf_grad( m::RBFModel, ‚Ñì :: Int64, x :: RVec )
    n_vars = length(m.training_sites[1]);

    difference_vectors = x_minus_sites(m,x); # array with len n_sites and entries n_vars
    distances = norm.(difference_vectors)       # n_sites x 1
    ‚àÇkernel_vals = ‚àÇkernel.( Val(m.kernel), distances, m.shape_parameter )  # n_sites

    coeff_‚Ñì = m.rbf_coefficients[:,‚Ñì]           # n_sites x 1
    grad_‚Ñì = zeros(eltype(x), n_vars);
    for i = 1 : length(distances)
        if distances[i] != 0
            grad_‚Ñì += coeff_‚Ñì[i] / distances[i] * ‚àÇkernel_vals[i] .* difference_vectors[i]
        end
    end
    return grad_‚Ñì
end

@doc "Compute the Jacobian matrix of the RBF part of the model"
function rbf_jacobian( m::RBFModel, x :: RVec )
    n_vars = n_in( m );
    n_out = num_outputs( m );

    difference_vectors = x_minus_sites(m,x); # array with len n_sites and entries n_vars
    distances = norm.(difference_vectors)       # n_sites x 1
    ‚àÇkernel_vals = ‚àÇkernel.( Val(m.kernel), distances, m.shape_parameter )  # n_sites

    rbf_jac = zeros(eltype(x), n_out, n_vars);
    for ‚Ñì = 1 : n_out
        coeff_‚Ñì = m.rbf_coefficients[:,‚Ñì]           # n_sites x 1
        grad_‚Ñì = zeros(eltype(x), n_vars);
        for i = 1 : length(distances)
            if distances[i] != 0
                grad_‚Ñì += coeff_‚Ñì[i] / distances[i] * ‚àÇkernel_vals[i] .* difference_vectors[i]
            end
        end
        rbf_jac[‚Ñì,:] = grad_‚Ñì;
    end
    return rbf_jac
end
rbf_jacobian( m::RBFModel, x :: Real ) = rbf_jacobian( m, [x])

# === Hessians of the model ===
# note ‚àÇ·µ¢ ‚àÇkernel(r(x)) = (x_i -c_i) * ‚àÇ‚àÇkernel(x)
‚àÇ‚àÇkernel( ::Val{:exp}, r :: Real, s :: Real = 1 ) = 2 * s^2 * (2* r^2 * s^2 - 1) * kernel( Val(:exp), r, s );
‚àÇ‚àÇkernel( ::Val{:multiquadric}, r :: Real, s :: Real = 1 ) = - s^2 / ((s*r)^2 + 1 )^(3/2) ;
‚àÇ‚àÇkernel( ::Val{:cubic}, r :: Real, s :: Real = 1 ) = 6 * s^3 * r;
‚àÇ‚àÇkernel( ::Val{:thin_plate_spline}, r :: Real, s :: Real = 1 ) = r == 0 ? 0 :
    (-1)^(s+1) * r^(2*s-2) *(
        4*s - 1 +
        2*(2*s-1)*s*log(r)
    );

function rbf_hessian( m::RBFModel, ‚Ñì :: Int64, x :: RVec )
    n_vars = length(m.training_sites[1])

    coeff_‚Ñì = m.rbf_coefficients[:, ‚Ñì]              # n_sites
    differences = x_minus_sites(m,x)        # n_sites
    distances = norm.( differences, 2 ); # n_sites

    ‚àÇkernel_eval = ‚àÇkernel.( Val(m.kernel), distances, m.shape_parameter )   # n_sites
    ‚àÇ‚àÇkernel_eval = ‚àÇ‚àÇkernel.( Val(m.kernel), distances, m.shape_parameter ) # n_sites

    rbf_hessian_‚Ñì = zeros( T, n_vars, n_vars )
    #rbf_hessian_‚Ñì .*= sum( coeff_‚Ñì .* ‚àÇkernel_eval )
    for i = 1 : length(coeff_‚Ñì)
        œÜ·µ¢‚Ä≤ = ‚àÇkernel_eval[i];
        œÜ·µ¢‚Ä≤‚Ä≤ = ‚àÇ‚àÇkernel_eval[i];
        coeff_‚Ñì[i]
        r·µ¢ = distances[i]
        D·µ¢ = differences[i];
        if r·µ¢ != 0
            ùöØ·µ¢ = ( (œÜ·µ¢‚Ä≤ / r·µ¢) .* I(n_vars) ) .+ ( ( œÜ·µ¢‚Ä≤‚Ä≤ - œÜ·µ¢‚Ä≤/r·µ¢ ) / r·µ¢^2  ) .* (D·µ¢ * D·µ¢')
        else
            ùöØ·µ¢ = œÜ·µ¢‚Ä≤‚Ä≤ .* I(n_vars)
        end
        rbf_hessian_‚Ñì .+= coeff_‚Ñì[i] .* ùöØ·µ¢
    end
    return rbf_hessian_‚Ñì
end

# === Evaluate polynomial part of the model ===
poly(m::RBFModel, ‚Ñì, x, ::Val{-1} ) = 0                       # degree -1 ‚áí No polynomial part (‚Ñì-th output)
poly(m::RBFModel, x, ::Val{-1} ) = 0                          # degree -1 ‚áí No polynomial part (all outputs)
poly(m::RBFModel, ‚Ñì, x, ::Val{0} ) = m.poly_coefficients[‚Ñì]     # degree 0 ‚áí constant (‚Ñì-th output)
poly(m::RBFModel, x, ::Val{0} ) = m.poly_coefficients[:]        # degree 0 ‚áí constant (all outputs)
poly(m::RBFModel, ‚Ñì, x, ::Val{1} ) = [x;1]'m.poly_coefficients[:, ‚Ñì] # affin linear tail (‚Ñì-th output)
poly(m::RBFModel, x, ::Val{1} ) = vec([x;1]'m.poly_coefficients)    # affin linear tail (all outputs)

@doc "Evaluate polynomial tail for output ‚Ñì."
function poly_output( m::RBFModel, ‚Ñì :: Int64, x :: RVec )
    poly( m, ‚Ñì, x, Val(m.polynomial_degree) )
end

@doc "k-Vector of evaluations of polynomial tail for all outputs."
function poly_output( m::RBFModel, x :: RVec )
    poly( m, x, Val(m.polynomial_degree) )
end

‚àápoly(m::RBFModel, ‚Ñì :: Int64 , ::Union{Val{-1}, Val{0} } ) = zeros( length( m.training_sites[1] ) )
‚àápoly(m::RBFModel, ‚Ñì :: Int64 , ::Val{1} ) = m.poly_coefficients[1 : end - 1, ‚Ñì];    # return all coefficients safe c_{n+1}
@doc "Gradient vector for polynomial tail of ‚Ñì-th output."
function poly_grad( m::RBFModel, ‚Ñì :: Int64 , x :: RVec )
    ‚àápoly( m, ‚Ñì, Val(m.polynomial_degree) )
end

function poly_jacobian( m::RBFModel, x :: RVec )
    hcat( [poly_grad(m, ‚Ñì, x) for ‚Ñì = 1 : length(m.training_values[1] ) ]... )'
end

#=
## For polynomials of degree at most 1 the hessian is always zero
poly_hessian( m::RBFModel, ‚Ñì::Int64, x::RVec)
=#

# === Combined model output ===
@doc "Evaluate ‚Ñì-th (scalar) model output at vector x."
function output( m::RBFModel, ‚Ñì :: Int64, x :: RVec )
    rbf_output( m, ‚Ñì, x ) + poly_output( m, ‚Ñì, x )
end
#output( m::RBFModel, ‚Ñì :: Int64, x :: Real ) = output(m, ‚Ñì, [x])

@doc "Evaluate all (scalar) model outputs at vector x and return k-vector of results."
function output( m::RBFModel, x :: RVec )
    vec(rbf_output( m, x ) .+ poly_output( m, x ))
end
#output( m::RBFModel, x :: Real ) = output(m, [x])

function grad( m::RBFModel, ‚Ñì :: Int64, x :: RVec )
    rbf_grad( m, ‚Ñì, x ) + poly_grad( m, ‚Ñì, x)
end
#grad(m::RBFModel, ‚Ñì::Int64, x::Real) = grad(m, ‚Ñì, [x])   # if n_vars == 1 and RBFModel is used outside of Optimization

function jac( m::RBFModel, x :: RVec )
    rbf_jacobian( m, x ) + poly_jacobian( m , x )
end
#jac(m::RBFModel, x::Real) = jac(m,[x])         # if n_vars == 1 and RBFModel is used outside of Optimization

function hess(m :: RBFModel, ‚Ñì :: Int64, x :: RVec)
    rbf_hessian(m,‚Ñì,x) # + poly_hessian == zeros
end
#hess(m::RBFModel, ‚Ñì::Int64, x::Real) = hess(m,‚Ñì,[x,])         # if n_vars == 1 and RBFModel is used outside of Optimization

# === Utiliy functions for solving the normal equations
# TODO right matrix types
get_Œ†( m :: RBFModel, ::Val{-1} ) =  Matrix{Real}( undef, 0, length(m.training_sites) );
get_Œ†( m :: RBFModel, ::Val{0} ) = ones(1, length(m.training_sites));
get_Œ†( m :: RBFModel, ::Val{1} ) = [ hcat( m.training_sites...); ones(1, length(m.training_sites)) ];

@doc "Return polynomial base matrix. Ones in last row."
function get_Œ†(m::RBFModel)
    get_Œ†( m, Val( m.polynomial_degree ) )
end

@doc "Return column vector to augment the polynomial base matrix `Œ†` if `y` were added."
function Œ†_col( m :: RBFModel , y :: RVec )
    pd = m.polynomial_degree
    if pd == -1
        return Matrix{Real}(undef, 0, 1)
    elseif pd == 0
        return 1
    elseif pd == 1
        return [ y ; 1 ]
    end
end

# see [WILD 2008]
function null_space_coefficients( Q, R, Z , L, f, Œ¶ )
    rhs = Z'f;
    w = L \ rhs;
    œâ = L' \ w;
    Œª = Z * œâ;  # RBF coefficients

    rhs_poly = Q'*( f.- Œ¶ * Œª)
    v = R \ rhs_poly;
    return Œª, v
end

@doc "Solve RBF linear equation system using Cholesky based null space method as in [WILD 2008]."
function solve_rbf_problem( Œ†, Œ¶, f, :: Val{true} )
    Q, R = qr( Œ†' );
    R = [
        R;
        zeros( size(Q,1) - size(R,1), size(R,2) )
    ]
    Z = Q[:, size(Œ†,1) + 1 : end ]

    ZŒ¶Z = Hermitian( Z'Œ¶*Z );   # ensure it is really Symmetric (rounding errors etc)
    #@show eigen(ZŒ¶Z).values
    try
        L = cholesky( ZŒ¶Z ).L     # should also be empty at this point
        Œª, v = null_space_coefficients( Q, R, Z, L, f, Œ¶)
        return vcat( Œª, v )

    catch PosDefException
        return solve_rbf_problem(Œ†,Œ¶,f,Val(false))
    end
end

@doc "Solve the RBF linear equation system using LU or QR factorization."
function solve_rbf_problem( Œ†, Œ¶, f, :: Val{false} )
    Œ¶_augmented = [ [Œ¶ Œ†']; [Œ† zeros(size(Œ†,1),size(Œ†,1) )] ];
    f_augmented = [ f;
                    zeros( size(Œ†,1), size(f,2) ) ];
    local Œ¶_fact;   # factorized matrix (for min norm solution if Œ¶ is singular)
    try
        Œ¶_fact = lu( Œ¶_augmented );
    catch SingularException
        @warn("\tWARNING RBF Matrix singular. Using QR decomposition.")
        Œ¶_fact = qr(Œ¶_augmented, Val(true))   # use QR to still obtain a (minimal norm) solution if Œ¶ is near singular
    end
    coefficients = Œ¶_fact \ f_augmented;   # obtain model coefficients as min norm solution to LGS

    return coefficients
end

# === Training functions ===
function set_coefficients!( m :: RBFModel, coefficients :: AbstractArray{R} where R<:Real)
    pd = m.polynomial_degree
    n_vars = n_in(m);
    n_out = num_outputs(m);
    n_sites = length( m.training_sites );
    if pd == -1
        m.rbf_coefficients = reshape(coefficients, (n_sites, n_out));
        m.poly_coefficients =  Matrix{Real}( undef, 0, n_out );
    elseif pd == 0
        m.rbf_coefficients = reshape(coefficients[1:end-1, :], (n_sites, n_out));
        m.poly_coefficients = reshape(coefficients[ end, : ], (1, n_out));
    elseif pd == 1
        m.rbf_coefficients = reshape(coefficients[1 : n_sites, :], n_sites, n_out);
        m.poly_coefficients = reshape(coefficients[ n_sites + 1 : end, : ], (n_vars + 1, n_out) );
    end # NOTE everything wrapped in reshape for the case that n_out == 1, n_vars == 1

end

@doc "Train (and fully instanciate) a RBFModel instance to best fit its training data."
function train!( m::RBFModel )
    Œ¶ = get_Œ¶( m );
    Œ† = get_Œ†( m );
    RHS = hcat( m.training_values... )';

    coefficients = solve_rbf_problem( Œ†, Œ¶, RHS, Val( is_valid(m) ) )
    set_coefficients!(m, coefficients)
    return m
end

# same as train! but using a null space method with data available from 'add_points!' method
# USE ONLY FOR MODELS WITH is_valid(m) == true !!!
function train!(m::RBFModel, Q , R, Z, L, Œ¶)

    RHS = hcat( m.training_values... )';
    Œª, v = null_space_coefficients( Q, R, Z, L, RHS, Œ¶)
    set_coefficients!(m, vcat(Œª,v))
    return m
end

function _test_interpolation( m :: RBFModel )
    for (i,site) in enumerate(m.training_sites)
        @assert all( isapprox.( m(site), m.training_values[i] ) )
    end
end

end#module
